There are one class "Crawl" and 14 functions.
Functions:
(1) __init__()
intialize all global parameters.

(2)downloadPage(url)
download the html page to a local path.

(3)getStartPage(query, number)
get the top 'number' of pages from google using the 'query' string.

(4)addMapping(Url, toUrl)
add mapping relation from Url to toUrl in the dictionary.

(5)findUrl(PageLimit)
the most important function. After getting start pages, call this function to start the crawling.

(6)getBaseUrl(url)
get the base url of the input url.

(7)formatFilter(url)
open the input url, check the mime type in the header. return 0 if failed or not html, return 1 if it's html.

(8)getRobotExlu(url)
input a url,  output 1 if get robots.txt and the url is allowed. output 0 if failed to get robots.txt or the url is disallowed.

(9)matrixGeneraor()
export a n*n matrix using the mapping dictionary. return the matrix for pagerank calculation.

(10)updateRankinfo(rank)
using the input rank to update the pagerank dictionary and the url waiting list (get higher rank pages).

(11)pageRank(Matrix)
input the matrix generated by function (9), output the pagerank in a list.

(12)getFinalRank()
this function is only used before output, the purpose is to update the final pagerank for each page.

(13)output()
export the information for each page (time, size, rank, final rank, etc.) into a csv file.

(14)handler()
rasie a exception when timeout.



How the program works:

For each url get during the crawling, save it to vistedList. prevent repeat.

First, call the function (3) to get the startpages. check each page's MIME format, if text/html, then this url is valid, save this url's information in to several global parameters:
	- urlWL: a queue to save all urls to be crawled in the future. All urls in the waiting list are valid and can be opened.
	- PG and PGreverse: save this two dictionary for fast lookup (using index or url)
	- urlInfo: urlInfor is a dictionary prepared for output. save the url's time and size into urlInfo after the first time get it.

Second, call function (5) to start the crawling.
	- when the number of total pages haven't exceeded the pagelimit, pop a url from waiting list, and get all links from this page. If success, save the father url into: crawledList. Means that this page is already crawled.

	- convert all relative urls to absolute urls and put these sublinks into a serial of filters (include format, if visited, if disallowed by robots.txt, if exceed the siteLimit, if login page, if file, etc.).

	- But check if the sublinks is already in the PG, if exist, means that the father url may point to another webpage which is already been saved. In this situation, add map relation using function(4)

	- All urls pass the filter are considered as valid html pages, because all of them is in a right format and can be opened. So these urls will be saved into: PG, PGreverse and urlInfo. Then add map relation using function(4) 

	- for each cycle, check if the number of pages reach the recalculation factor, if reach, trigger the pagerank update process:
		- call function(9) to export a matrix from mapping dictionary
		- pass this matrix to function(11) to calculate the rank
		- update rank dictionary and url waiting list using the new page rank.
		- save the rank into urlInfo if there isn't a estimate rank.
		- break the small loop and go to the next url from waiting list.

	- if url waiting list is empty, trigger the same pagerank update process.
	- if url waiting list is still empty, end the crawling.

- Each page saved into PG, PGreverse, urlInfo is valid and can be opened. All invalid pages, 404 pages are discarded in the filter. So each PGID corresponding to one valid url. The index PGID can be used to find the rank for each page after calculation.

- If there is no more page to crawled Or the number of page reach the limit, end the crawling. Update the rank again for the current pages, save all final pagerank into urlInfo.

- Export urlInfo and statistic data into a csv file using function(13)


Non-working features:
- If network is disconnected, the all data lose.
- The comma incluede in the url may affect the csv output.
- Cgi page and login page may not be recongnized if there isn't 'account', 'cgi', 'login' in the url.
- If the parameters mentioned in the readme is not set correctlly, the final result may not reach 1000.


Additional sources:
http://blog.sina.com.cn/s/blog_63041bb80102uy5o.html
https://stackoverflow.com/questions/19833440/unicodeencodeerror-ascii-codec-cant-encode-character-u-xe9-in-position-7
https://stackoverflow.com/questions/12474406/python-how-to-get-the-content-type-of-an-url
https://docs.python.org/2/library/urlparse.html#urlparse.urlsplit
https://en.wikipedia.org/wiki/PageRank
https://gist.github.com/diogojc/1338222/84d767a68da711a154778fb1d00e772d65322187
https://docs.python.org/2/library/urllib.html
https://stackoverflow.com/questions/5909/get-size-of-a-file-before-downloading-in-python


Special features:
- Self manually timeout: there is a timeout parameter in requests.get() and urllib2.urlopen(), but other functions may still be stucked by some reason, so a timeout feature which can be used anywhere is useful to prevent this kind of situations.
- Recalculate triggers: there are two recalculating triggers, one is based on the visited pages, the other is based on the waiting list. This two triggers make sure the content in the waiting list.
- PRTop10.py can get the top 10 Or top N from the output csv file.
